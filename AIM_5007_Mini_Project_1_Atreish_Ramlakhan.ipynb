{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Atreish Ramlakhan\n",
    "AIM 5007: Neural Networks and Deep Learning\n",
    "Fall 2021\n",
    "Mini-Project #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Klength</th>\n",
       "      <th>Kwidth</th>\n",
       "      <th>Asym_coeff</th>\n",
       "      <th>length of kernel groove</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  Compactness  Klength  Kwidth  Asym_coeff  \\\n",
       "0  15.26      14.84       0.8710    5.763   3.312       2.221   \n",
       "1  14.88      14.57       0.8811    5.554   3.333       1.018   \n",
       "2  14.29      14.09       0.9050    5.291   3.337       2.699   \n",
       "3  13.84      13.94       0.8955    5.324   3.379       2.259   \n",
       "4  16.14      14.99       0.9034    5.658   3.562       1.355   \n",
       "\n",
       "   length of kernel groove  class_label  \n",
       "0                    5.220            1  \n",
       "1                    4.956            1  \n",
       "2                    4.825            1  \n",
       "3                    4.805            1  \n",
       "4                    5.175            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/atreish/AIM_5007_Neural_Networks_-_Deep_Learning/main/seeds_dataset.txt',\n",
    "                   sep='\\s+',header=None, names = ['Area','Perimeter','Compactness','Klength','Kwidth','Asym_coeff','length of kernel groove','class_label'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Numeric Data, Not the label column\n",
    "data.iloc[:,:-1] = data.iloc[:,:-1].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "#Shuffle the dataset to ensure the network is trained on all 3 available classes\n",
    "data1 = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Change X  (numeric columns) and y (class columns) to arrays and the target datatype to int\n",
    "X = np.array(data1)[:, :-1]\n",
    "y = np.array(data1)[:, -1].astype(int)\n",
    "\n",
    "#Apply one hot encoding for each class, this will be our target class \n",
    "target_layer = []\n",
    "for i in y:\n",
    "    if i == 1:\n",
    "        target_layer.append([1,0,0])\n",
    "    elif i == 2:\n",
    "        target_layer.append([0,1,0])\n",
    "    else:\n",
    "        i == 3\n",
    "        target_layer.append([0,0,1])\n",
    "        \n",
    "target_layer = np.array(target_layer)\n",
    "\n",
    "#Train-test split 150/60\n",
    "X_train = X[:150]\n",
    "X_test = X[150:]\n",
    "y_train = target_layer[:150]\n",
    "y_test = target_layer[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Activation Function for this network'''\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "'''Append the entire matrix with a column of ones for the bias node'''\n",
    "\n",
    "def append(X):\n",
    "    height = X.shape[0] \n",
    "    bias_column = np.ones((height,1))\n",
    "    X_appended = np.hstack((X,bias_column))\n",
    "    return X_appended\n",
    "\n",
    "'''Initializes a Weight Matrix from the input layer to the hidden layer using the length of a single row \n",
    "for argument Matrix X'''\n",
    "\n",
    "def w_input(X):\n",
    "    X_appended = append(X)\n",
    "    rows = len(X[0]) \n",
    "    cols = len(X[0])\n",
    "    W = np.random.sample((cols, rows)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight, rows)\n",
    "    W_input = np.insert(W, cols, bias_row, axis=0)\n",
    "    return W_input\n",
    "\n",
    "'''This is the dot product between a weight matrix and the appended X matrix (one row at a time), this returns\n",
    "an unactivated hidden layer of length 7, as specified by the dimensions of the problem'''\n",
    "\n",
    "def h_raw(X,i,weight1=None): \n",
    "    if weight1 is None:\n",
    "        weight1 = w_input(X)\n",
    "    X_appended_row = append(X)[i]\n",
    "    hraw = np.dot(X_appended_row, weight1)\n",
    "    return hraw, weight1\n",
    "\n",
    "'''This function now activates the hidden layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the input layer to the hidden layer'''\n",
    "\n",
    "def h_activated(X,i,weight1=None):\n",
    "    hidden_layer_raw, weight1 = h_raw(X,i,weight1)\n",
    "    hactivated = sigmoid(hidden_layer_raw)\n",
    "    return hactivated, weight1\n",
    "\n",
    "'''Initializes a Weight Matrix for the hidden to output layer using the length of the activated hidden layer,\n",
    "this also appends a random weight at the bottom of the matrix for the bias from hidden to output layer'''\n",
    "\n",
    "def w_output(X, i, y_train, weight1=None):\n",
    "    hactivated, _ = h_activated(X,i,weight1=weight1)\n",
    "    rows = len(hactivated)\n",
    "    W = np.random.sample((rows, 3)) - 0.5\n",
    "    random_weight = np.random.sample([1]) - 0.5\n",
    "    weight_vector = np.repeat(random_weight,3)\n",
    "    W_output = np.insert(W, rows, weight_vector, axis=0)\n",
    "    return W_output\n",
    "\n",
    "'''This is the dot product between an appended activated hidden layer and W_output, this gives us the first\n",
    "output layer values'''\n",
    "\n",
    "def o_raw(X,i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X, i , weight1)\n",
    "    h_activated_with_bias = np.append(hactivated,[1]) \n",
    "    if weight2 is None:\n",
    "        weight2 = w_output(X,i,weight1)\n",
    "    oraw = np.dot(h_activated_with_bias, weight2)\n",
    "    return oraw, weight2\n",
    "\n",
    "'''This function now activates the output layer by applying the sigmoid activation function and returns the \n",
    "weight that was used to get from the hidden layer to the output layer'''\n",
    "\n",
    "def o_activated(X,i, weight1=None, weight2=None):\n",
    "    oraw, weight2 = o_raw(X,i, weight1=weight1, weight2=weight2)\n",
    "    oactivated = sigmoid(oraw)\n",
    "    return oactivated, weight2\n",
    "\n",
    "'''This Error function determines the error of the output for the i-th row of the Matrix and the i-th target layer\n",
    "and outputs a single value that will be used in the backpropogation steps'''\n",
    "\n",
    "def error(X,y,i,weight1=None, weight2=None):\n",
    "    oactivated, _ = o_activated(X, i, weight1=weight1, weight2=weight2)\n",
    "    error = .5*((y[i] - oactivated)**2).sum()\n",
    "    return error\n",
    "\n",
    "'''The Feedforward Steps all conglomerated into one function from X matrix row to output activated. This catch-all\n",
    "function will return the outputlayer actived for the i-th row, the weight matricies used form both the input layer\n",
    "to hidden layer and from the hidden layer to the input layer and the associated erro'''\n",
    "\n",
    "def feedforward(X, y, i, weight1=None, weight2=None):\n",
    "    hactivated, weight1 = h_activated(X,i, weight1)\n",
    "    oactivated, weight2 = o_activated(X,i, weight1=weight1, weight2=weight2)\n",
    "    err = error(X,y,i,weight1=weight1, weight2=weight2)\n",
    "    return oactivated, hactivated, err, weight1, weight2\n",
    "\n",
    "'''First Step in Backpropogation, this produces the Gradient Matrix from the Hidden Layer to the\n",
    "Output Layer (8x3)'''\n",
    "\n",
    "def grad_hidden_to_output(X,y,i, oactivated, hactivated): \n",
    "    E = (oactivated - y[i]) * oactivated * (1 - oactivated)\n",
    "    h_activated_transpose = np.array([[i] for i in hactivated])\n",
    "    grad_hidden_to_output = h_activated_transpose * E \n",
    "    bias_gradient = ((oactivated - y[i]) * oactivated * (1 - oactivated)).sum()\n",
    "    bias_h2o =  np.array([bias_gradient])\n",
    "    len_bias_row = grad_hidden_to_output.shape[1]\n",
    "    BG_H_O = np.repeat(bias_h2o[0],len_bias_row)\n",
    "    hidden_to_output_gradient_matrix = np.vstack([grad_hidden_to_output, BG_H_O])\n",
    "    return hidden_to_output_gradient_matrix\n",
    "\n",
    "'''The Gradient Matrix from the Input Layer to the Hidden Layer (8x7)'''\n",
    "\n",
    "def grad_input_to_hidden(X, y, i, oactivated, hactivated, W_output):\n",
    "    input_layer = X[i]\n",
    "    target_layer = y[i]\n",
    "    W_output_wo_bias = W_output[:-1,:] \n",
    "    E = (oactivated - target_layer) * oactivated * (1 - oactivated) \n",
    "    grad_input_to_hidden = np.zeros((7,7))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E, W_output_wo_bias[j]) ) \n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    pt3 = np.dot(E, W_output_wo_bias.T)\n",
    "    pt4 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt3,pt4)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(bias_gradient_il, len_bias_row)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((grad_input_to_hidden, BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix\n",
    "\n",
    "'''Updated Weights from the Input to Hidden Layer with learning rate lr'''\n",
    "\n",
    "def update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=None, weight2=None):\n",
    "    input_to_hidden_gradient_matrix = grad_input_to_hidden(X, y, i, oactivated, hactivated, weight2)\n",
    "    updated_weights_input_hidden = weight1 - lr * input_to_hidden_gradient_matrix\n",
    "    return updated_weights_input_hidden\n",
    "\n",
    "'''Updated Weights for Hidden to Output Layer with lr'''\n",
    "\n",
    "def update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2=None):\n",
    "    hidden_to_output_gradient_matrix =  grad_hidden_to_output(X,y,i, oactivated, hactivated)\n",
    "    updated_weights_hidden_output = weight2 - lr * hidden_to_output_gradient_matrix\n",
    "    return updated_weights_hidden_output\n",
    "\n",
    "'''Backpropogation Function'''\n",
    "\n",
    "def backpropogation(X,y,i,lr,oactivated,hactivated,weight1=None, weight2=None):\n",
    "    updated_weight1 = update_input_hidden_weights(X,y,i,lr, oactivated, hactivated, weight1=weight1, weight2=weight2)\n",
    "    updated_weight2 = update_hidden_output_weights(X,y,i,lr, oactivated, hactivated, weight2)\n",
    "    oactivated1, hactivated1, err, updated_weight1, updated_weight2 = feedforward(X, y, i, weight1=updated_weight1, weight2=updated_weight2)\n",
    "    return err, oactivated1, updated_weight1, updated_weight2\n",
    "\n",
    "'''Transforms the Rows of all oactivated1 to one hot encoding to compare with y_trian'''\n",
    "\n",
    "def output_encoder(Matrix):\n",
    "    for row in Matrix:\n",
    "        row[row.argmax(0)] = 1\n",
    "        row[row < 1] = 0\n",
    "    return Matrix\n",
    "\n",
    "'''Creates a visualization of the Errors'''\n",
    "\n",
    "def error_plotter(Matrix,threshold): \n",
    "    result = np.array(list(map(sum, Matrix))) / len(Matrix[0])\n",
    "    index = np.array(list(range(len(result))))\n",
    "    Ezz = np.vstack((index+1, result)).T\n",
    "    x = Ezz[:,0]\n",
    "    y = Ezz[:,1]\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.plot(x,y)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This Neural Network Training function will apply our defined functions above to the dataset of our choice.\n",
    "The seeds data has already been shuffled and split. In this function, we will iterate through the rows of our\n",
    "Matrix input, forward-propograte and then backward-propogate, obtain updated weights and then apply the forward\n",
    "propogation again to the same row, that is for each row. At every stage,  '''\n",
    "\n",
    "def nn_training(X, y, lr, threshold=.001, epoch_count=1000):\n",
    "    start = timer()\n",
    "    wt1 = None\n",
    "    wt2 = None\n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    \n",
    "    for n in range(epoch_count):\n",
    "        \n",
    "        M = np.concatenate((X_train, y_train), axis=1)\n",
    "        M = pd.DataFrame(M)\n",
    "        M1 = M.sample(frac=1).reset_index(drop=True)\n",
    "        XX_train = np.array(M1.iloc[:,:7])\n",
    "        yy_train = np.array(M1.iloc[:,7:])\n",
    "        \n",
    "        avg_error = []       \n",
    "        for i in range(XX_train.shape[0]):  \n",
    "            oact, hact, err, wt1, wt2 = feedforward(XX_train,yy_train,i,weight1=wt1, weight2=wt2)\n",
    "            err, oactivated1, updated_weight1, updated_weight2 = backpropogation(XX_train,yy_train,i,lr,oactivated=oact,hactivated=hact,weight1=wt1, weight2=wt2)\n",
    "            wt1 = updated_weight1\n",
    "            wt2 = updated_weight2\n",
    "            final_output.append(oactivated1)\n",
    "            errors.append(err)\n",
    "            avg_error.append(err)\n",
    "    #    if np.array(avg_error).mean() < threshold:\n",
    "    #        break\n",
    "     #       return error_plotter2(avg_error.mean(),threshold)\n",
    "    \n",
    "    epoch_errors = np.reshape(errors,(epoch_count, len(X)))\n",
    "    normalized_errors = np.array(list(map(sum,  epoch_errors))) / len( epoch_errors[0])\n",
    "    error_plotter(epoch_errors,threshold)\n",
    "    \n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == yy_train, axis=1))\n",
    "    accuracy = tot/len(y_train)\n",
    "\n",
    "    end = timer()\n",
    "    time_elspased = end-start\n",
    "    \n",
    "    print('\\nAfter {} epochs, the accuracy is {}%'.format(epoch_count,round(accuracy*100, 2)))\n",
    "    print('\\nTime Used for this function {} seconds'.format(round(time_elspased,2)))\n",
    "    print('\\nFinal Error = {}'.format(round(normalized_errors[-1],5)))\n",
    "    print('\\nFinal Input Layer to Hidden Layer Weights:\\n\\n{}'.format(wt1))\n",
    "    print('\\n\\nFinal Hidden Layer to Output Layer Weights:\\n\\n{}'.format(wt2))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 3000 epochs, the accuracy is 100.0%\n",
      "\n",
      "Time Used for this function 414.79 seconds\n",
      "\n",
      "Final Error = 0.00126\n",
      "\n",
      "Final Input Layer to Hidden Layer Weights:\n",
      "\n",
      "[[ 0.42921828 -2.66524711  1.19818713 -1.51536613 -4.42580162 -2.02583819\n",
      "  -8.09253752]\n",
      " [ 0.66475767 -2.33187799  0.83218281 -2.00122838 -3.89710475 -1.54981842\n",
      "  -9.62880575]\n",
      " [-0.22923743  1.34537326 -1.38403856 -0.86402761  4.3868439   1.05056251\n",
      "   0.84785982]\n",
      " [-0.63459638  5.89125631 -2.20574868  1.66612353 11.55205683  9.77342735\n",
      "  -7.21960652]\n",
      " [ 0.80790807 -2.50173193  0.51124751 -1.80902167 -3.16397669 -1.08687541\n",
      "  -7.47020664]\n",
      " [-1.68943276 -0.50452941 -3.40436241  0.17918559 -3.58174553 -6.16661982\n",
      "   5.18432531]\n",
      " [ 0.60698823 -4.10130766  2.21559746 -2.61152821 -7.74766857 -8.24071259\n",
      "   9.77743214]\n",
      " [ 2.57993327  2.57993327  2.57993327  2.57993327  2.57993327  2.57993327\n",
      "   2.57993327]]\n",
      "\n",
      "\n",
      "Final Hidden Layer to Output Layer Weights:\n",
      "\n",
      "[[ -4.74221523   6.18615374  -1.82620772]\n",
      " [  5.79961613  -4.46954369   1.09292891]\n",
      " [ -6.70918087   6.63861363  -2.18812044]\n",
      " [  1.60526833  -2.70164446   0.28253794]\n",
      " [ 10.49585249 -10.38727524  -2.34993075]\n",
      " [  8.37979786  -7.9409275   -6.21367849]\n",
      " [-13.2194537   -4.91282617  13.69378793]\n",
      " [ -0.98751585  -0.98751585  -0.98751585]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Apply the above training program using defined functions to the provided X_train and y_train data. This data\n",
    "is length 150. After running we will have time spent running, accuracy, final Error (from ALL errors),\n",
    "a visualization of the mean of the Error for every epoch, in this case 3000 and the Weight Matricies to be used \n",
    "on the testing set. The point of the visualization is to determine from the datapoints plotted if the Error \n",
    "indeed seems to be heading downward or there is no real change'''\n",
    "\n",
    "nn_training(X_train, y_train, lr=.1, threshold=.01, epoch_count=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Testing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following code will be very similar to our training data, however here we will input the weights obtained\n",
    "above by the Training Function after 3000 epochs. These weights will ideally have allow our testing dataset's \n",
    "error to converge rapidly to the threshold. Testing is a bit different, firstly, only the feedforward \n",
    "function is applied to the datapoint (rows) of our testing dataframe. Accuracy is determined for the activated\n",
    "output layer. As before the metric for testing the output layer with our y_test is the label encoder which applies\n",
    "a 1 to the highest value and a 0 to others. This mimics the one hot encoding application done to the classes \n",
    "that are held in the dataframe. If the activated-output layer matches the y_test row, it is a match. This entire\n",
    "testing function is applied ONCE to the validation dataset, there are no epochs in testing. The intent here is\n",
    "to return an accuracy metric and timer for the weights provided by the above training function'''\n",
    "\n",
    "def nn_testing(X, y):\n",
    "    start = timer()\n",
    "    \n",
    "    wt1 = np.array([[ 0.42921828, -2.66524711,  1.19818713, -1.51536613, -4.42580162, -2.02583819,\n",
    "  -8.09253752],\n",
    " [ 0.66475767, -2.33187799,  0.83218281, -2.00122838, -3.89710475, -1.54981842,\n",
    "  -9.62880575],\n",
    " [-0.22923743,  1.34537326, -1.38403856, -0.86402761,  4.3868439,   1.05056251,\n",
    "   0.84785982],\n",
    " [-0.63459638,  5.89125631, -2.20574868,  1.66612353, 11.55205683,  9.77342735,\n",
    "  -7.21960652],\n",
    " [ 0.80790807, -2.50173193,  0.51124751, -1.80902167, -3.16397669, -1.08687541,\n",
    "  -7.47020664],\n",
    " [-1.68943276, -0.50452941, -3.40436241,  0.17918559, -3.58174553, -6.16661982,\n",
    "   5.18432531],\n",
    " [ 0.60698823, -4.10130766,  2.21559746, -2.61152821, -7.74766857, -8.24071259,\n",
    "   9.77743214],\n",
    " [ 2.57993327,  2.57993327,  2.57993327,  2.57993327,  2.57993327,  2.57993327,\n",
    "   2.57993327]])\n",
    "    \n",
    "    wt2 = np.array([[ -4.74221523,   6.18615374,  -1.82620772],\n",
    " [  5.79961613,  -4.46954369,   1.09292891],\n",
    " [ -6.70918087,   6.63861363,  -2.18812044],\n",
    " [  1.60526833,  -2.70164446,   0.28253794],\n",
    " [ 10.49585249, -10.38727524,  -2.34993075],\n",
    " [  8.37979786,  -7.9409275,   -6.21367849],\n",
    " [-13.2194537,   -4.91282617,  13.69378793],\n",
    " [ -0.98751585,  -0.98751585,  -0.98751585]])\n",
    "    \n",
    "    final_output = []\n",
    "    errors = [] \n",
    "    for i in range(X.shape[0]):  \n",
    "        oact, hact, err, wt1, wt2 = feedforward(X,y,i,weight1=wt1, weight2=wt2)\n",
    "        final_output.append(oact)\n",
    "        errors.append(err)\n",
    "    output = np.asarray(final_output[-len(X):])\n",
    "    encoded_output = output_encoder(output)      \n",
    "    tot = np.sum(np.all(encoded_output == y_test, axis=1))\n",
    "    accuracy = tot/len(y_test)\n",
    "    end = timer()\n",
    "    time_elspased = end-start\n",
    "    print('\\nThe accuracy is {}%'.format(round(accuracy*100, 2)))\n",
    "    print('\\nTime Used for this function {} seconds'.format(round(time_elspased,2)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy is 91.67%\n",
      "\n",
      "Time Used for this function 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "nn_testing(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
