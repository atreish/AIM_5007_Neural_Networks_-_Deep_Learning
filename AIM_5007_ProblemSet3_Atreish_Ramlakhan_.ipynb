{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Atreish Ramlakhan\n",
    "AIM 5007: Neural Networks and Deep Learning\n",
    "Fall 2021\n",
    "Problem Set #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 0**\n",
    "\n",
    "```Write a function that takes dimensions as inputs and creates an initial set of weights in a matrix of those\n",
    "dimensions. (For the example above, you will need to create an 8×7 matrix for 𝑊𝑖𝑛𝑝𝑢𝑡 as well as another 8×3 matrix for 𝑊ℎ𝑖𝑑𝑑𝑒𝑛.) Each entry should be a randomly selected nonzero but small value, say in the range (−0.5 , 0.5). There is one caveat. The final row of the matrix will hold the weights for that layer’s bias weights, and these need to be the same value for each entry. After each pass (or batch, if we choose a batch update approach), we will be updating this set of weights, so the initialization will only need to be done once for each matrix. Hence, problem zero!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_input(rows,cols):\n",
    "    W = np.random.sample((rows-1, cols)) - 0.5\n",
    "    random_bias_weight = np.random.sample()\n",
    "    bias_row = np.repeat(random_bias_weight,cols)\n",
    "    W_input = np.insert(W, rows-1, bias_row, axis=0)\n",
    "    return W_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**\n",
    "\n",
    "```Write a function that takes a vector of length 𝑚 as input and returns a vector of length 𝑚 + 1, where\n",
    "the output vector is the original vector plus an extra 1 appended as the final entry.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append(v):\n",
    "    v = np.array(v)\n",
    "    return np.append(v,[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**\n",
    "\n",
    "```Given the vector 𝑓 of inputs, we need to use weights in 𝑊𝑖𝑛𝑝𝑢𝑡 to obtain the raw, incoming values for\n",
    "the hidden layer nodes. In other words, we need to calculate:\n",
    "ℎ𝑟𝑎𝑤 = 𝑎𝑝𝑝𝑒𝑛𝑑(𝑓)𝑇*𝑊𝑖𝑛𝑝𝑢𝑡\n",
    "Write a function that takes the properly appended feature vector 𝑓 and multiplies its transpose by the\n",
    "weight matrix 𝑊𝑖𝑛𝑝𝑢𝑡 to get ℎ𝑟𝑎𝑤. (What are the dimensions of this resulting vector?)\n",
    "Note that you should incorporate your function from problem 1 into the function for problem 2.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hraw(input_layer,W_input):\n",
    "    hraw = np.dot(append(input_layer),W_input)\n",
    "    return hraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**\n",
    "\n",
    "``We next need to activate the neurons in our hidden layer. This means, loosely, we need:\n",
    "ℎ𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑 = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(ℎ𝑟𝑎𝑤)\n",
    "Write a function that takes a vector of raw values and outputs a vector of transformed values, where the\n",
    "transformation is performed using the sigmoid function on each element of the vector individually.\n",
    "What are the dimensions of the output vector with respect to the input vector?``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def h_activated_(hidden_layer):\n",
    "    hactivated = sigmoid(hidden_layer)\n",
    "    return hactivated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**\n",
    "\n",
    "```We must begin the backward propagation process by calculating our error. Write a function that takes\n",
    "the final, activated output vector and calculates its error with respect to the true vector of one-hot\n",
    "encodings for this observation. Use the squared error loss function we used before:\n",
    "𝐸 = 0.5 ∗ ∑(𝑡𝑎𝑟𝑔𝑘 − 𝑇𝑜𝑢𝑡𝑘)^2\n",
    "In vector terms we can express this as:\n",
    "𝐸 = 0.5 ∙ (𝑜𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑 − 𝑡)𝑇(𝑜𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑 − 𝑡)\n",
    "Concept check: Why do I calculate 𝑜𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑 − 𝑡 instead of 𝑡 − 𝑜𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑒𝑑? Does it matter?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Concept check: the order does not matter as the abs(b-a) = abs(a-b) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w_output(rows,length_of_output_layer):\n",
    "    W = np.random.sample((rows-1, length_of_output_layer)) - 0.5\n",
    "    random_weight = np.random.sample([1]) - 0.5\n",
    "    weight_vector = np.repeat(random_weight,length_of_output_layer)\n",
    "    W_output = np.insert(W, rows-1, weight_vector, axis=0)\n",
    "    return W_output\n",
    "\n",
    "def o_raw(hactivated,W_output):\n",
    "    h_activated_with_bias = append(hactivated)\n",
    "    oraw = np.dot(h_activated_with_bias,W_output)\n",
    "    return oraw\n",
    "\n",
    "def o_activated(oraw):\n",
    "    o_activated = sigmoid(oraw)\n",
    "    return o_activated\n",
    "\n",
    "'''Concept check: the order does not matter as the abs(b-a) = abs(a-b) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(o_activated, target_layer):\n",
    "    error = .5 * ((target_layer - o_activated)**2).sum()\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**\n",
    "\n",
    "``Write a function that calculates the gradient of the weight from a hidden node to an output node. In\n",
    "Bramer’s notation, this is 𝑔(𝐸, 𝑊𝑗𝑘). Note that you will need to make use of the various quantities that\n",
    "are defined above. Make sure you include all necessary inputs to your function.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden_to_output(hactivated, o_activated, target_layer): \n",
    "    E = (o_activated - target_layer) * o_activated * (1 - o_activated)\n",
    "    h_activated_transpose = np.array([[i] for i in hactivated])\n",
    "    grad_hidden_to_output = h_activated_transpose * E \n",
    "    return grad_hidden_to_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**\n",
    "\n",
    "``Write a function that calculates the gradient of the weight from the bias term to the output nodes. In\n",
    "Bramer’s notation, this is 𝑔(𝐸, 𝑏𝑖𝑎𝑠𝑂).``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_hidden_bias_to_output(o_activated,target_layer):\n",
    "    bias_gradient = ((o_activated - target_layer) * o_activated * (1 - o_activated)).sum()\n",
    "    return np.array([bias_gradient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_to_output_gradient_matrix(o_activated,target_layer,hactivated):\n",
    "    grad_hid_2_output = grad_hidden_to_output(hactivated, o_activated, target_layer)\n",
    "    grad_hid_bias_2_out =  gradient_hidden_bias_to_output(o_activated,target_layer)\n",
    "    len_bias_row = grad_hid_2_output.shape[1]\n",
    "    BG_H_O = np.repeat(grad_hid_bias_2_out[0],len_bias_row)\n",
    "    hidden_to_output_gradient_matrix = np.vstack([grad_hid_2_output, BG_H_O])\n",
    "    return hidden_to_output_gradient_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**\n",
    "\n",
    "`` Write a function that calculates the gradient of the weight from an input node to a hidden node. In\n",
    "Bramer’s notation, this is 𝑔(𝐸, 𝑤𝑖𝑗). 𝑔(𝐸,𝑤) = ∑ [(𝑇𝑜𝑢𝑡𝑘−𝑡𝑎𝑟𝑔𝑘)∙𝑇𝑜𝑢𝑡𝑘∙(1−𝑇𝑜𝑢𝑡𝑘)∙𝑊 ]∙𝑇h𝑖𝑑𝑗∙(1−𝑇h𝑖𝑑𝑗)∙𝑖𝑛𝑝𝑖 ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_input_to_hidden(hactivated, o_activated, target_layer, input_layer, W_output):\n",
    "    W_output_wo_bias = W_output[:-1,:]\n",
    "    E = (o_activated - target_layer) * o_activated * (1 - o_activated) \n",
    "    grad_input_to_hidden = np.zeros((7,7))\n",
    "    for j in range(len(hactivated)):\n",
    "        pt1 = []  \n",
    "        pt2 = []\n",
    "        pt1.append( np.dot(E,W_output_wo_bias[j]) )\n",
    "        pt2.append( hactivated[j] * (1 - hactivated[j])) \n",
    "        grad_input_to_hidden[:,j] = np.array(pt1) * np.array(pt2) * input_layer\n",
    "    return grad_input_to_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**\n",
    "\n",
    "``Write a function that calculates the gradient of the weight from the bias term to the hidden nodes. In\n",
    "Bramer’s notation, this is 𝑔(𝐸,𝑏𝑖𝑎𝑠𝐻) = ∑[(𝑇𝑜𝑢𝑡𝑘−𝑡𝑎𝑟𝑔𝑘)∙𝑇𝑜𝑢𝑡𝑘∙(1−𝑇𝑜𝑢𝑡𝑘)∙𝑊 ]∙𝑇h𝑖𝑑𝑗∙(1−𝑇h𝑖𝑑𝑗)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_input_bias_to_hidden(o_activated,target_layer,hactivated,W_output):\n",
    "    W_output_wo_bias = W_output[:-1,:]\n",
    "    E = (o_activated - target_layer) * o_activated * (1-o_activated)\n",
    "    pt1 = np.dot(E,W_output_wo_bias.T)\n",
    "    pt2 = hactivated *(1-hactivated)\n",
    "    bias_gradient_il = np.dot(pt1,pt2) \n",
    "    return bias_gradient_il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_hidden_gradient_matrix(input_layer,o_activated,target_layer,hactivated,W_output):\n",
    "    W_output_wo_bias = W_output[:-1,:]\n",
    "    input_to_hidden_gradient_bias =  gradient_input_bias_to_hidden(o_activated,target_layer,hactivated,W_output)\n",
    "    len_bias_row = hactivated.shape[0]\n",
    "    BG_I_H = np.repeat(input_to_hidden_gradient_bias,len_bias_row)\n",
    "    G_I_H = grad_input_to_hidden(hactivated,o_activated,target_layer,input_layer,W_output)\n",
    "    input_to_hidden_gradient_matrix = np.vstack((G_I_H,BG_I_H))\n",
    "    return input_to_hidden_gradient_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 9**\n",
    "\n",
    "``Write a function that takes as inputs a learning rate (𝛼) and updates the weights for the input-tohidden-node step. Note that you need to update the bias weights so that it is the same weight for each\n",
    "node. You will probably want to incorporate your functions from problems 7-8 here.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_input_hidden_weights(lr, input_to_hidden_gradient_matrix):\n",
    "    updated_weights_input_hidden = input_to_hidden_gradient_matrix - lr * input_to_hidden_gradient_matrix\n",
    "    return updated_weights_input_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 10**\n",
    "\n",
    "``Write a function that takes as inputs a learning rate (𝛼) and updates the weights for the hidden-to-output\n",
    "node step. Note that you need to update the bias weights so that it is the same weight for each\n",
    "node. You will probably want to incorporate your functions from problems 5-6 here.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hidden_output_weights(lr, hidden_to_output_gradient_matrix):\n",
    "    updated_weights_hidden_output = hidden_to_output_gradient_matrix - lr * hidden_to_output_gradient_matrix\n",
    "    return updated_weights_hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 11**\n",
    "\n",
    "```We now should have all the pieces that we need to train our model. Write a function that performs one\n",
    "entire pass based on a single training instance, using your functions above. What should the output(s) of\n",
    "this function be?\n",
    "Hint: What would we need from here to run our model on a new instance to make a prediction?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(input_layer,target_layer,lr):\n",
    "    \n",
    "        #weights from input to hidden layer\n",
    "        W_input = w_input(len(input_layer)+1,len(input_layer))\n",
    "        \n",
    "        #hidden layer \n",
    "        h_raw = hraw(input_layer,W_input)\n",
    "        hactivated = h_activated_(h_raw)\n",
    "     \n",
    "        #weights from hidden to output layer\n",
    "        W_output = w_output(len(hactivated)+1,len(target_layer))\n",
    "        \n",
    "        #output layer\n",
    "        oraw = o_raw(hactivated,W_output)\n",
    "        oactivated = o_activated(oraw)\n",
    "        \n",
    "        #Error\n",
    "        error1 = error(oactivated, target_layer)\n",
    "        \n",
    "        #Hidden to Output Gradient Matrix\n",
    "        ho_gm = hidden_to_output_gradient_matrix(oactivated,target_layer,hactivated)\n",
    "        \n",
    "        #Input to Hidden Gradient Matrix\n",
    "        ih_gm = input_to_hidden_gradient_matrix(input_layer,oactivated,target_layer,hactivated,W_output) \n",
    "        \n",
    "        #Updated Weights Input to Hidden\n",
    "        updated_weights_input_hidden = update_input_hidden_weights(lr,ih_gm)\n",
    "        \n",
    "        #Updated Weights Hidden to Output\n",
    "        updated_weights_hidden_output = update_hidden_output_weights(lr,ho_gm)\n",
    "        \n",
    "        #Round 2 \n",
    "        hraw2 = hraw(input_layer, updated_weights_input_hidden)\n",
    "        hactivated2 = h_activated_(hraw2) \n",
    "        oraw2 = o_raw(hactivated2, updated_weights_hidden_output)\n",
    "        oactivated2 = o_activated(oraw2)\n",
    "        error2 = error(oactivated2, target_layer)\n",
    "        \n",
    "        #Print Results\n",
    "        print('The Input Layer:\\n\\n{}\\n\\n{}'.format(input_layer,input_layer.shape))\n",
    "        print('\\nThe Input Layer with Bias:\\n\\n{}\\n\\n{}'.format(append(input_layer),append(input_layer).shape))\n",
    "        print('\\nThe Input to Hidden Layer Weights:\\n\\n{}\\n\\n{}'.format(W_input,W_input.shape))\n",
    "        print('\\nThe Updated Input to Hidden Layer Weights:\\n\\n{}\\n\\n{}'.format(updated_weights_input_hidden,updated_weights_input_hidden.shape))\n",
    "        print('\\nActivated Hidden Layer:\\n\\n{}\\n\\n{}'.format(hactivated,hactivated.shape))\n",
    "        print('\\nActivated Hidden Layer with Bias:\\n\\n{}\\n\\n{}'.format(append(hactivated),append(hactivated).shape))\n",
    "        print('\\n2nd Acitivated Hidden Layer:\\n\\n{}\\n\\n{}'.format(hactivated2,hactivated2.shape))\n",
    "        print('\\n2nd Activated Hidden Layer with Bias:\\n\\n{}\\n\\n{}'.format(append(hactivated2,),append(hactivated2).shape))\n",
    "        print('\\nThe Hidden to Output Layer Weights:\\n\\n{}\\n\\n{}'.format(W_output,W_output.shape))\n",
    "        print('\\nThe Hidden to Output Layer Weights:\\n\\n{}\\n\\n{}'.format(updated_weights_hidden_output,updated_weights_hidden_output.shape))\n",
    "        print('\\nTarget Layer:\\n\\n{}\\n\\n{}'.format(target_layer,target_layer.shape))\n",
    "        print('\\nOutput Activated:\\n\\n{}\\n\\n{}'.format(oactivated,oactivated.shape))\n",
    "        print('\\nError:\\n\\n{}'.format(error1))\n",
    "        print('\\nOutput Activated after first Epoch:\\n\\n{}\\n\\n{}'.format(oactivated2,oactivated2.shape))\n",
    "        print('\\nNew Error:\\n\\n{}'.format(error2))\n",
    "        print('\\nDifference in Error:\\n\\n{}'.format((error2-error1)))\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Input Layer:\n",
      "\n",
      "[0.8  0.5  0.2  0.4  0.1  0.9  0.05]\n",
      "\n",
      "(7,)\n",
      "\n",
      "The Input Layer with Bias:\n",
      "\n",
      "[0.8  0.5  0.2  0.4  0.1  0.9  0.05 1.  ]\n",
      "\n",
      "(8,)\n",
      "\n",
      "The Input to Hidden Layer Weights:\n",
      "\n",
      "[[ 0.26117775  0.24955745  0.05239375  0.03716968 -0.30364393 -0.28100587\n",
      "  -0.11204826]\n",
      " [ 0.45685888 -0.37722857 -0.35178791  0.09734848  0.06872325  0.31335523\n",
      "   0.33949041]\n",
      " [ 0.27691861 -0.38827277  0.11226109 -0.48072102  0.38510449 -0.13817043\n",
      "   0.13122025]\n",
      " [ 0.0570362  -0.20135073  0.39340942 -0.26880682 -0.18955664 -0.07893268\n",
      "  -0.09680542]\n",
      " [-0.48693779 -0.26485398 -0.37113303  0.39351482 -0.40851947  0.25747275\n",
      "   0.22550512]\n",
      " [-0.33213158  0.42732106 -0.32121571  0.32397543  0.24169247  0.41458053\n",
      "   0.12409748]\n",
      " [ 0.47870973 -0.03751717  0.18317469 -0.29711467 -0.1139993  -0.02398624\n",
      "   0.26655153]\n",
      " [ 0.94522838  0.94522838  0.94522838  0.94522838  0.94522838  0.94522838\n",
      "   0.94522838]]\n",
      "\n",
      "(8, 7)\n",
      "\n",
      "The Updated Input to Hidden Layer Weights:\n",
      "\n",
      "[[-1.28803030e-03  1.15688518e-02  1.77514733e-02  9.08759053e-03\n",
      "   1.61699298e-02  1.71970237e-03 -2.29633657e-03]\n",
      " [-8.05018936e-04  7.23053240e-03  1.10946708e-02  5.67974408e-03\n",
      "   1.01062062e-02  1.07481398e-03 -1.43521036e-03]\n",
      " [-3.22007574e-04  2.89221296e-03  4.43786833e-03  2.27189763e-03\n",
      "   4.04248246e-03  4.29925594e-04 -5.74084142e-04]\n",
      " [-6.44015148e-04  5.78442592e-03  8.87573666e-03  4.54379527e-03\n",
      "   8.08496492e-03  8.59851187e-04 -1.14816828e-03]\n",
      " [-1.61003787e-04  1.44610648e-03  2.21893416e-03  1.13594882e-03\n",
      "   2.02124123e-03  2.14962797e-04 -2.87042071e-04]\n",
      " [-1.44903408e-03  1.30149583e-02  1.99704075e-02  1.02235393e-02\n",
      "   1.81911711e-02  1.93466517e-03 -2.58337864e-03]\n",
      " [-8.05018936e-05  7.23053240e-04  1.10946708e-03  5.67974408e-04\n",
      "   1.01062062e-03  1.07481398e-04 -1.43521036e-04]\n",
      " [ 6.58914763e-02  6.58914763e-02  6.58914763e-02  6.58914763e-02\n",
      "   6.58914763e-02  6.58914763e-02  6.58914763e-02]]\n",
      "\n",
      "(8, 7)\n",
      "\n",
      "Activated Hidden Layer:\n",
      "\n",
      "[0.75715075 0.76029416 0.66240198 0.75695277 0.71276364 0.77128206\n",
      " 0.76140951]\n",
      "\n",
      "(7,)\n",
      "\n",
      "Activated Hidden Layer with Bias:\n",
      "\n",
      "[0.75715075 0.76029416 0.66240198 0.75695277 0.71276364 0.77128206\n",
      " 0.76140951 1.        ]\n",
      "\n",
      "(8,)\n",
      "\n",
      "2nd Acitivated Hidden Layer:\n",
      "\n",
      "[0.51569791 0.52337003 0.52705569 0.52189012 0.52611315 0.51749352\n",
      " 0.51509586]\n",
      "\n",
      "(7,)\n",
      "\n",
      "2nd Activated Hidden Layer with Bias:\n",
      "\n",
      "[0.51569791 0.52337003 0.52705569 0.52189012 0.52611315 0.51749352\n",
      " 0.51509586 1.        ]\n",
      "\n",
      "(8,)\n",
      "\n",
      "The Hidden to Output Layer Weights:\n",
      "\n",
      "[[ 0.03975106 -0.47891766 -0.419987  ]\n",
      " [-0.40622478  0.30937224  0.02818835]\n",
      " [-0.4017711   0.00069345 -0.43794965]\n",
      " [-0.15358935  0.05122131 -0.32130252]\n",
      " [-0.28691141  0.22848335 -0.33310981]\n",
      " [-0.11441595 -0.12025063 -0.10250654]\n",
      " [-0.23674884 -0.25209414  0.1331076 ]\n",
      " [ 0.49035602  0.49035602  0.49035602]]\n",
      "\n",
      "(8, 3)\n",
      "\n",
      "The Hidden to Output Layer Weights:\n",
      "\n",
      "[[-0.08966318  0.08458648 -0.08915605]\n",
      " [-0.09003543  0.08493766 -0.0895262 ]\n",
      " [-0.07844286  0.07400145 -0.07799919]\n",
      " [-0.08963974  0.08456437 -0.08913274]\n",
      " [-0.08440678  0.0796277  -0.08392938]\n",
      " [-0.09133664  0.08616519 -0.09082004]\n",
      " [-0.09016751  0.08506226 -0.08965753]\n",
      " [-0.12445705 -0.12445705 -0.12445705]]\n",
      "\n",
      "(8, 3)\n",
      "\n",
      "Target Layer:\n",
      "\n",
      "[1 0 1]\n",
      "\n",
      "(3,)\n",
      "\n",
      "Output Activated:\n",
      "\n",
      "[0.34438795 0.56963181 0.36478482]\n",
      "\n",
      "(3,)\n",
      "\n",
      "Error:\n",
      "\n",
      "0.5789029426578562\n",
      "\n",
      "Output Activated after first Epoch:\n",
      "\n",
      "[0.39077341 0.54414782 0.39120384]\n",
      "\n",
      "(3,)\n",
      "\n",
      "New Error:\n",
      "\n",
      "0.5189433307281465\n",
      "\n",
      "Difference in Error:\n",
      "\n",
      "-0.05995961192970978\n"
     ]
    }
   ],
   "source": [
    "il = np.array([.8,.5,.2,.4,.1,.9,.05])\n",
    "t = np.array([1,0,1])\n",
    "epoch(il,t,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
